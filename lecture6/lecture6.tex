\documentclass[12pt]{article}
\usepackage{amsfonts, epsfig}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{calc,positioning}
\usetikzlibrary{shapes,arrows}
\pagestyle{fancy}
\lfoot{\texttt{coms10013.github.io}}
\lhead{Analysis - 6 optimization - Conor}
\rhead{\thepage}
\cfoot{}
\begin{document}

\section*{Optimization}
One common application of analysis relevant to computer science is
optimization. These are frequently of use in problems where computer
scientists are working with people from other disciplines rather than
working on problems that arise out of computer science itself; this
includes engineering, science and finance, these days the most obvious
application is in data science.


In a typical optimization problem the goal is to minimize some
function, $E$ that depends on lots of parameters, so $E(\mathbf{x})$ where
\begin{equation}
\mathbf{x}=(x_1,x_2,\ldots,x_n)
\end{equation}
$E$ is often called an \textsl{objective function} or \textsl{loss
  function} and the idea is to find the values of $\mathbf{x}$ that
minimize $E$.

This problem occurs in a diversity of situations. In physics $E$ might
be an energy function related to the configuration of a mechanical
system with $\mathbf{x}$ representing the positions of the components,
or, for a particle physicist, a configuration of fields like the
electromagnetic field or its generalizations with $\mathbf{x}$
representing field values. For someone working on machine learning it
might measure the error made by a neural network, in this situation
$E$ is called an error function and $\mathbf{x}$ are the parameters
describing the neural network; the parameters often labelled with $w$s
that describe the connection strengths in the network. In statistics
$E$ might be the log-likelihood, a measure of how well a statistical
model predicts the data. In neuroscience, the model might be a
computational model of a neuron or synapse and $\mathbf{x}$ would
correspond to the components of that model, for example the number of
each of the different types of ion gates. In this example optimization
the model to fit electro-physiological recording will predict the
composition of the neuron or synapse.

In any case, the idea is to find the value of $\mathbf{x}$ that gives
the lowest value of $E(\mathbf{x})$. The best way of doing this
depends on the dimension of $\mathbf{x}$, on how much is known about
$E$, on its properties and how easy it is to calculate. The hardest
optimization problems are often ones where $E$ has many local minima
so that optimization algorithms end up finding these and missing the
global minimum. This isn't a problem we are going to discuss here in
detail, but we will note in passing that it is a feature of the loss
function for machine learning which is, to some extent, solved by
stochastic gradient descent.

\subsubsection*{Gradient descent}


Obviously, if we know the functional form of $E$ we can find its minimum
using calculus. For example, say
\begin{equation}
E(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2
\end{equation}
we would calculate the two partial derivatives and then find the
optimum point by setting them to zero, so
\begin{equation}
\begin{aligned}
0&=\frac{\partial E}{\partial x_1}=2(1-x_1)-400x_1(x_2-x_1^2)\\
0&=\frac{\partial E}{\partial x_2}=200(x_2-x_1^2)
\end{aligned}
\end{equation}
which is solved by $x_1=x_2=1$. However, this isn't the sort of
problem we are thinking about here, we are envisaging situations where
$E$ cannot be treated analytically like this, so, for example, $E$
itself must be calculated numerically for a given value of
$\mathbf{x}$.

Another approach might be to divide the space of $\mathbf{x}$ values
up into a grid and to check them all; this is known as grid-search and
is sometimes the best method for problems where $\mathbf{x}$ has only
a small number of dimensions and $E$ is very quick to
calculate. However, $E$ might be slow to calculate, for example, in
neuroscience examples, the model might take quite a while to
run. Furthermore, the number of grid points may be very large,
particularly if the variability of $E$ means the grid has to be fine,
or if the number of dimensions is not small. Often grid search is
impractical or out of the question.

Another popular set of methods is related to gradient descent. Roughly
speaking this involves working out which direction is down and heading
that way. The gradient is a vector pointing in the direction $E$
increases most, the simplest gradient method is to start at an initial
value $\mathbf{x}_0$ and iterate
\begin{equation}
\mathbf{x}_{n+1}=\mathbf{x}_n-\eta \mathbf{\nabla}E
\end{equation}
where ${\nabla}E$ is the gradient and $\eta$ is a, usually small, step
size. Now, if the functional form of $E$ is known, the gradient can be
worked out using calculus, and even in situations where $E$ has to be
calculated numerically it may be possible to find a similar numerical
scheme to calculate the gradient. This is often the case for error
functions in machine learning, for example. However, it is not always
the case that the gradient can be easily estimated; in problems coming
from neuroscience, for example, the models are highly non-linear and
there is no easy way to find the gradient.

Even if the gradient is known, gradient descent methods can be
tricky. One problem is with the size of $\eta$: the scale of the $E$
landscape can change; often if $\mathbf{x}$ is a long way away from
the optimal value $E$ is very smooth so it would be useful to take
long steps, but near the minimum the features become much finer, so
small steps are needed. If $\eta$ is too small the method spends for
ages taking tiny steps down the hill side, if it is too big it steps
right over important features at the bottom. Another problem is with
thin valleys, if the step size is too large then often successive
steps oscillate backwards and forwards across the valley rather than
following it down to the minimum. There are solutions to these
problems, for example the method called conjugate gradients seeks to
choose the successive steps to follow the gradient but to avoid the
sort of repetitive backtracking that can happen at thin valleys.

\subsubsection*{Neural networks}

In the case of machine learning, the loss function is usually
minimized using stochastic gradient descent. The loss function is
typically a measure of how well the true outcome was predicted by the
network. A common measure is cross-entropy loss: in a network
performing a classification problem the output layer might give
probabilities for each of the possible classes and the cross-entropy
loss for input $\textbf{u}$ is
\begin{equation}
  L(\textbf{u};\theta)=-\log{p_*}-\sum_{i\not=*}\log(1-p_i)
\end{equation}
where $p_*$ is the probability the network assigns to the correct
class and $p_i$ for $i\not=*$ is the probability it assigns to the
other classes. Here we have used $\textbf{u}$ for the input to the neural
network, for example the pixel values in an image classification
problem, this is potentially confusing since in the machine learning
literature we often use $\textbf{x}$ for inputs, but I didn't want to
do that here since above that's the name we give for the parameters we
are optimizing; however, to follow machine learning tradition those
parameters are called $\theta$ here. 

We aren't going to discuss why cross entropy is a good choice of loss
function or how the logarithms appear, suffice to say the choice is
well motivated by information theory. The important thing though is to
understand the loss function depends on parameters, $\theta$ that
describe the network, these are thought of as connection strengths
between nodes and effective describe matrix multiplications that form
part of the transformation from input, $\textbf{u}$, to output, the
$p_i$'s that estimate the probability of different classes.

Note, however, that the loss above is only calculated for one input-output pair;
the goal, in a sense, would be to minimize the ``global loss'':
\begin{equation}
  L(\theta)=\frac{1}{N}\sum_{\mbox{all possible inputs}}L(\textbf{u};\theta)
\end{equation}
where $N$ is some sort of mythical number counting all possible
inputs. Of course, we don't have all possible input and, in any case,
if we knew the pairings of all possible inputs and the corresponding
class, we wouldn't need to do machine learning. In practice we have
only the training set; we will leave aside considerations of
over-training, though they are important, this suggests we should minimize
\begin{equation}
  L(\theta,\rm{data})=\frac{1}{n}\sum_{\mbox{all data}}L(\textbf{u};\theta)
\end{equation}
where $n$ is the amount of data. It seems like a good strategy to
calculate this on the data set, work out the gradients of
$L(\theta,\rm{data})$ with respect to the parameters we are lumping
together under the name $\theta$ and then do gradient descent. In
fact, it works better to break the data up into smaller, random, sets
called batches and do gradient descent in turn on the loss for each batch:
\begin{equation}
  L(\theta,\rm{batch})=\frac{1}{r}\sum_{\mbox{data in this batch}}L(\textbf{u};\theta)
\end{equation}
where $r$ is the batch size. This has two advantages, the first is
purely practical, working with batches makes the autograd and matrix calculations
easier for working out the gradient, but the second is theoretical,
this introduces a bit of noisiness into the gradient calculation; the
noise is the ``stochastic'' in stochastic gradient descent. This noise
may make the optmization work better by helping the descent find its
ways along narrow valleys and hopping it out of shallow local minima or plateaux.

We don't want to spend too much time about this, except to wrap up by
noting that a lot of though has been given in machine learning to the
other issue we noted with gradient flow, the problem of setting the
learning rate. In machine learning there are a collection of
algorithms, such as AdaGrad, RMSProp and most famously Adam that set
good learning rates for each parameter.


\subsubsection*{Downhill simplex}

The problem of optimization is a rich and complicated one; here we
will consider in detail one simple approach: the downhill simplex or
Nelder-Mead method. For many problems it will not be the fastest way
to find the optimum in terms of run time or the number of function
evaluations, but it is often a method that works without having to
learn too much about the properties of the objective function; in
other words, it is often the quickest way to solve the problem if you
include the researchers' time along with the time it takes to run. It
also does not require gradient information.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\coordinate (x1) at (0,0); \coordinate (x2) at (5,0); \coordinate (x3)
at (2.5,4.33); \node[draw=red](x1n) at (x1) {$x_1$};
\node[draw=red](x2n) at (x2) {$x_2$}; \node[draw=red](x3n) at (x3)
     {$x_3$}; \draw (x1n) -- (x2n); \draw (x2n) -- (x3n); \draw (x3n)
     -- (x1n);
\end{tikzpicture}
\end{center}
\caption{A simplex is a generalization of a triangle to higher numbers
  of dimensions, but here the 2-dimensional simplex is illustrated and
  this is just a triangle. A simplex is not regular in general but in
  many of these figures they are shown as regular to make things look
  clearer. In the description of the downhill simplex algorithm the
  vertices are ordered so $E(\mathbf{x}_1)\le E(\mathbf{x}_2)\le \ldots
  E(\mathbf{x}_{n+1})$.\label{fig:simplex}}
\end{figure}

A simplex is a generalization of a triangle, just as a triangle is
defined in two-dimensional space and has three vertices and a
tetrahedron is defined in three-dimensional space and has four
vertices, a $n$-dimensional simplex has $(n+1)$-vertices. Here we will
be considering simplices in the space of parameter values $\mathbf{x}$
so the vertices will be
$\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{n+1}\}$. Simplices are
not regular in general, the distances between the vertices need not be
the same, but it is required that it isn't flat: if you remove one
vertex the remaining $n$ vertices should be linearly independent.

Now the idea of downhill simplex is that a simplex kind of ooches
around looking for the minimum. It does this, roughly speaking, by
taking the worst vertex, the one with the largest value of $E$, and
replacing it with a better one. There are a number of different
operations that achieve this, which is used depends on the 
situation.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\coordinate (x1) at (0,0); 
\coordinate (x2) at (5,2); 
\coordinate (x3) at (0,7); 
\coordinate (xo) at (2.5,1);
\coordinate (xr) at (5,-5);
\node[draw=red](x1n) at (x1) {$x_1$};
\node[draw=red](x2n) at (x2) {$x_2$}; 
\node[draw=red](x3n) at (x3) {$x_3$}; 
\node[draw=red](xon) at (xo) {$x_o$}; 
\node[draw=red](xrn) at (xr) {$x_r$}; 
\draw (x1n) -- (xon); 
\draw (xon) -- (x2n); 
\draw (x2n) -- (x3n); 
\draw (x3n) -- (x1n);
\draw[dashed] (x1n) -- (xrn);
\draw[dashed] (x2n) -- (xrn);
\end{tikzpicture}

\end{center}
\caption{The reflected point is constructed by reflecting the point
  with the worst value of the objective function through the centroid
  of the other points.\label{fig:reflection}}
\end{figure}

For simplicity let's imagine the vertices are in order according to the
objective function, so
\begin{equation}
E(\mathbf{x}_1)\le E(\mathbf{x}_2)\le \ldots \le E(\mathbf{x}_{n+1})
\end{equation}
This order is done at the start of each iteration, though obvious in
practice this doesn't involve renumbering, that is just done here for
notational convenience. The idea generally is to replace
$\mathbf{x}_{n+1}$. It is useful to define the centroid of the
remaining points:
\begin{equation}
\mathbf{x}_o=\frac{1}{n}\sum_{i=1}^n\mathbf{x}_i
\end{equation}
Now let $\mathbf{x}_r$ be the point you get to by reflecting
$\mathbf{x}_{n+1}$ in the $n$-dimensional simplex formed by the
remaining points. It is
\begin{equation}
\mathbf{x}_r=\mathbf{x}_o+(\mathbf{x}_o-\mathbf{x}_{n+1})
\end{equation}
Often this will be a sort of \lq{}reasonable point\rq{} so that if it replaced
$\mathbf{x}_{n+1}$ it would not be the best nor the worst, that is
\begin{equation}
E(\mathbf{x}_1)\le E(\mathbf{x}_r) < E(\mathbf{x}_n)
\end{equation}
In this case use it is used to replace $\mathbf{x}_{n+1}$:
\begin{equation}
\mathbf{x}_{n+1}\leftarrow \mathbf{x}_r
\end{equation}
This is called reflection and is shown in
Fig.\ref{fig:reflection}.

There are other cases. The reflected point $x_r$ might be better than all
of the existing points, so
\begin{equation}
E(\mathbf{x}_r)<E(\mathbf{x}_1)
\end{equation}
in which case it is worth trying to go even further in the same
direction by considering the so called expansion point $\mathbf{x}_e$:
\begin{equation}
\mathbf{x}_e=\mathbf{x}_o+2(\mathbf{x}_o-\mathbf{x}_{n+1})
\end{equation}
If $E(\mathbf{x}_e)<E(\mathbf{x}_r)$ then
\begin{equation}
\mathbf{x}_{n+1}\leftarrow \mathbf{x}_e
\end{equation}
otherwise
\begin{equation}
\mathbf{x}_{n+1}\leftarrow \mathbf{x}_r
\end{equation}
as before. The expansion point is illustrated in
Fig.~\ref{fig:refexp}. One advantage of expansion is that it makes the
simplex bigger; since going from the reflection point to the expansion
point gives an even lower value of the objective function it is likely
that the relevant features of the landscape are larger than the
simplex. This is one advantage of the simplex approach, the size of
the simplex changes to match scale of detail in the landscape.

\begin{figure}
\begin{center}
\begin{tikzpicture}


\coordinate (a) at (0,14);
\node at (a) {\textbf{(a)}};

\coordinate (b) at (8,14);
\node at (b) {\textbf{(b)}};


\coordinate (x1) at (0,9.33);
\coordinate (x2) at (5,9.33);
\coordinate (x3) at (2.5,13.66);
\coordinate (x0) at (2.5,9.33);
\coordinate (xr) at (2.5,5);

\node[draw=red](x1n) at (x1) {$x_1$};
\node[draw=red](x2n) at (x2) {$x_2$};
\node[draw=red](x3n) at (x3) {$x_3$};
\node[draw=red](x0n) at (x0) {$x_o$};
\node[draw=red](xrn) at (xr) {$x_r$};

\draw (x1n) -- (x0n);
\draw (x0n) -- (x2n);

\draw (xrn) -- (x2n);
\draw (x1n) -- (xrn);

\draw[dashed] (x2n) -- (x3n);
\draw[dashed] (x3n) -- (x1n);

\coordinate (x1r) at (8,9.33);
\coordinate (x2r) at (13,9.33);
\coordinate (x3r) at (10.5,13.66);
\coordinate (x0r) at (10.5,9.33);
\coordinate (xer) at (10.5,0.77);

\node[draw=red](x1nr) at (x1r) {$x_1$};
\node[draw=red](x2nr) at (x2r) {$x_2$};
\node[draw=red](x3nr) at (x3r) {$x_3$};
\node[draw=red](x0nr) at (x0r) {$x_o$};
\node[draw=red](xenr) at (xer) {$x_e$};

\draw (x1nr) -- (x0nr);
\draw (x0nr) -- (x2nr);

\draw (xenr) -- (x2nr);
\draw (x1nr) -- (xenr);

\draw[dashed] (x2nr) -- (x3nr);
\draw[dashed] (x3nr) -- (x1nr);

\end{tikzpicture}
\end{center}

\caption{This illustrates reflection \textbf{(a)} and expansion
  \textbf{(b)}. In reflection the worst point is reflected through
  $\mathbf{x}_0$, the center of the remaining points to give a new
  point $\mathbf{x}_r$. Expansion goes from $\mathbf{x}_0$ to
  $\mathbf{x}_r$ and continues the same distance again in the same
  direction to give $\mathbf{x}_e$\label{fig:refexp} }
\end{figure}

Another possibility is that the reflection point would be the worst
point if it replaced $\mathbf{x}_{n+1}$:
\begin{equation}
E(\mathbf{x}_r)>E(\mathbf{x}_n)
\end{equation}
Replacing $x_{n+1}$ with $x_r$ in this situation would lead to
wasteful repetitive moves. It also implies that the simplex is too
big, reflecting the worst point causes it to skip over a region around
the points $\{\mathbf{x_1},\mathbf{x_2},\ldots,\mathbf{x}_n\}$ where
there are lower values of the objective function. To deal with this a
new point is defined, the contraction point
\begin{equation}
\mathbf{x}_c=\mathbf{x}_o-(\mathbf{x}_o-\mathbf{x}_{n+1})/2
\end{equation}
If $E(\mathbf{x}_c)<E(\mathbf{x}_{n+1})$ then
\begin{equation}
\mathbf{x}_{n+1}\leftarrow \mathbf{x}_c
\end{equation}
Finally, if this does not work, there is an emergency move called
reduction which shrinks the whole simplex towards the best point. In
this move all the points except $\mathbf{x}_1$ are replaced by the
point half-way to $\mathbf{x}_1$ so
\begin{equation}
\mathbf{x}_i=(\mathbf{x}_i+\mathbf{x}_1)/2
\end{equation}
for $i=2,3,\ldots,(n+1)$. Reduction and contraction are both
illustrated in Fig.~\ref{fig:conred} and a flow chart for the whole
algorithm is given in Fig.~\ref{fig:flowchart}.

Obviously after any of these moves the points are reordered, or in
fact the best, worst and second worst points identified, and the
algorithm repeats. However, practically, a stopping criterion needs to
be devised. This usually says that the algorithm stops when the
simplex vertices have values of the objective function that are
extremely close to each other and are unchanging from iteration to
iteration, where \lq{}extremely\rq{} here stands for some specified
tolerance. Often a maximum number of iterations is also specified in
case the algorithm doesn't converge.

\begin{figure}
\begin{center}
\begin{tikzpicture}

\coordinate (a) at (0,5);
\node at (a) {\textbf{(a)}};

\coordinate (b) at (8,5);
\node at (b) {\textbf{(b)}};


\coordinate (x1) at (0,0);
\coordinate (x2) at (5,0);
\coordinate (x3) at (2.5,4.33);
\coordinate (x0) at (2.5,0);
\coordinate (xc) at (2.5,2.17);

\node[draw=red](x1n) at (x1) {$x_1$};
\node[draw=red](x2n) at (x2) {$x_2$};
\node[draw=red](x3n) at (x3) {$x_3$};
\node[draw=red](x0n) at (x0) {$x_o$};
\node[draw=red](xcn) at (xc) {$x_c$};

\draw (x1n) -- (x0n);
\draw (x0n) -- (x2n);

\draw (xcn) -- (x2n);
\draw (x1n) -- (xcn);

\draw[dashed] (x2n) -- (x3n);
\draw[dashed] (x3n) -- (x1n);

\coordinate (x1) at (8,0);
\coordinate (x2) at (13,0);
\coordinate (x3) at (10.5,4.33);
\coordinate (x2c) at (10.5,0);
\coordinate (x3c) at (9.25,2.17);

\node[draw=red](x1n) at (x1) {$x_1$};
\node[draw=red](x2n) at (x2) {$x_2$};
\node[draw=red](x3n) at (x3) {$x_3$};
\node[draw=red](x2cn) at (x2c) {$x'_{2}$};
\node[draw=red](x3cn) at (x3c) {$x'_{3}$};

\draw (x1n) -- (x2cn);
\draw (x1n) -- (x3cn);
\draw (x2cn) -- (x3cn);

\draw (x2cn)[dashed] -- (x2n);
\draw (x3cn)[dashed] -- (x3n);
\draw (x2n)[dashed] -- (x3n);


\end{tikzpicture}
\end{center}
\caption{Contraction \textbf{(a)} and reduction \textbf{(b)}. In
  contraction a new point $\mathbf{x}_c$ is chosen which is half way
  between the worst point $\mathbf{x}_{n+1}$ and $\mathbf{x}_0$, the
  centre of the remaining points. In reduction the triangle is shrunk
  to half its size while keeping the best point.\label{fig:conred}}
\end{figure}


% Define block styles
\tikzstyle{decision} = [diamond, draw, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse, node distance=3cm,
    minimum height=2em] 


\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (order){order $\mathbf{x}_1$ $\ldots$ $\mathbf{x}_{n+1}$};
    \node [block, below of= order] (reflect){reflect: $\mathbf{x}_r$};
    \node[decision, below of= reflect] (xr){is $\mathbf{x}_r$ the best point};
    \node[block, right=1cm of xr] (expand){expand: $\mathbf{x}_e$};
    \node[decision, right = 1cm of expand](xe){$E(\mathbf{x}_e)<E(\mathbf{x}_r)$};
    \node[cloud,above of=xe](xn12xe){$\mathbf{x}_{n+1}\leftarrow \mathbf{x}_e$};
    \node[cloud,below of=xe](xn12xr){$\mathbf{x}_{n+1}\leftarrow \mathbf{x}_r$};
    \node[decision,below of=xr](xragain){$E(\mathbf{x}_r)>E(\mathbf{x}_n)$};
    \node[block,below =1cm of xragain](contract){contract: $\mathbf{x}_c$};
    \node[decision,right =0.7cm of contract](xc){$E(\mathbf{x}_c)<E(\mathbf{x}_{n+1})$};
    \node[cloud,right =0.7cm of xc](xn12xc){$\mathbf{x}_{n+1}\leftarrow \mathbf{x}_c$};
    \node[cloud,below of= xc](reduce){reduce};

    % Draw edges
    \path [line] (order) -> (reflect);
    \path [line] (reflect) -> (xr);

    \path [line] (expand) -> (xe);
    \path [line] (xragain) -> node{yes}(contract);
    \path[line] (contract) -> (xc);
    \path [line] (xr) -> node{yes}(expand);    
    \path [line] (xr) -> node{no}(xragain);    
    \path [line] (xe) -> node {yes} (xn12xe);
    \path [line] (xe) -> node {no} (xn12xr);
    \path [line] (xragain) -> node {no} (xn12xr);
    \path [line] (xc) -> node {yes} (xn12xc);
    \path [line] (xc) -> node {no} (reduce);
\end{tikzpicture}
\end{center}
\caption{A flow chart for the downhill simplex algorithm. At the
  start, at the top, the points are put in order so that
  $\mathbf{x}_{n+1}$ is the worst point and $\mathbf{x}_1$ is the
  best. Next the reflected point is calculated to give $\mathbf{x}_r$
  and $f(\mathbf{x}_r)$ is calculated. By comparing to the
  $f(\mathbf{x_i})$ it can be decided if $\mathbf{x}_r$ is the best
  point, that is $f(\mathbf{x}_r)<f(\mathbf{x}_1)$ and the flow chart
  has two branches depending on the answer. This carries out until an
  oval is reached, one or more points are changed and the process
  repeats.\label{fig:flowchart}}
\end{figure}

The downhill simplex is a simple, intuitively appealing and robust
algorithm. It is easy to code. It is often the thing that you try to
see if you can avoid doing something cleverer and hence
trickier. However, there are better algorithms, often the best
algorithm that like this that don't use gradient information is
Powell's method which involve choosing a direction and optimizating
the objective function in that direction, there are very good
algorithms for minimizing functions of one variable. Another direction
is then chosen and the one-dimensional optimization is repeated, and
so on until the algorithm converges. The details of the algorithms
account for how the directions are chosen and how the one-dimensional
optimization is performed.

A big problem is what happens when there is more than one minimum: the
local minima that act as a trap for the simplex on its way to the
global minimum. One approach to this is to set the simplex off from
lots of different initial conditions in the hope that one of them will
reach the absolute minimum. Another method is to use something like
simulated annealing or the genetic algorithm, these are designed to
work well when the answer is hiding behind lots of local minima, but
tends to be slow and finickity.


\section*{Summary}
In optimization we find the parameter values, or maybe a, minimum for
an objective function. Of this is done using gradient descent:
\begin{equation}
\mathbf{x}_{n+1}=\mathbf{x}_n-\eta \mathbf{\nabla}E
\end{equation}
Stochastic gradient descent, for example, forms the basis for deep
learning neural networks. For lots of complicated problems were the
gradient is not available, downhill simplex works.


\end{document}

